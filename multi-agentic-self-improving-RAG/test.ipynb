{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ae0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777af91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e367fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required environment variables not set. Please set them in your .env file or environment.\n"
     ]
    }
   ],
   "source": [
    "# This is a critical check. We verify that our script can access the necessary API keys from the environment.\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ or \"ENTREZ_EMAIL\" not in os.environ:\n",
    "    # If the keys are missing, we print an error and halt, as the application cannot proceed.\n",
    "    print(\"Required environment variables not set. Please set them in your .env file or environment.\")\n",
    "else:\n",
    "    # This confirmation tells us our secrets have been loaded securely and are ready for use.\n",
    "    print(\"Environment variables loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299ebf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We explicitly set the LangSmith project name. This is a best practice that ensures all traces\n",
    "\n",
    "# generated by this project are automatically grouped together in the LangSmith user interface for easy analysis.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AI_Clinical_Trials_Architect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc6d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05277de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8607f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4b/f46tx7f94tddgdzqfc22ctw80000gn/T/ipykernel_10703/2096888050.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  \"planner\": ChatOllama(model=\"gpt-oss:20b-cloud\", temperature=0.0, format='json'),\n",
      "/var/folders/4b/f46tx7f94tddgdzqfc22ctw80000gn/T/ipykernel_10703/2096888050.py:17: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  \"embedding_model\": OllamaEmbeddings(model=\"snowflake-arctic-embed:22m\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This dictionary will act as our central registry, or \"foundry,\" for all LLM and embedding model clients.\n",
    "llm_config = {\n",
    "    # For the 'planner', we use Llama 3.1 8B. It's a modern, highly capable model that excels at instruction-following.\n",
    "    # We set `format='json'` to leverage Ollama's built-in JSON mode, ensuring reliable structured output for this critical task.\n",
    "    \"planner\": ChatOllama(model=\"gpt-oss:20b-cloud\", temperature=0.0, format='json'),\n",
    "    \n",
    "    # For the 'drafter' and 'sql_coder', we use Qwen2 7B. It's a nimble and fast model, perfect for\n",
    "    # tasks like text generation and code completion where speed is valuable.\n",
    "    \"drafter\": ChatOllama(model=\"deepseek-v3.1:671b-cloud\", temperature=0.2),\n",
    "    \"sql_coder\": ChatOllama(model=\"deepseek-v3.1:671b-cloud\", temperature=0.0),\n",
    "    \n",
    "    # For the 'director', the highest-level strategic agent, we use the powerful Llama 3 70B model.\n",
    "    # This high-stakes task of diagnosing performance and evolving the system's own procedures\n",
    "    # justifies the use of a larger, more powerful model.\n",
    "    \"director\": ChatOllama(model=\"gpt-oss:120b-cloud\", temperature=0.0, format='json'),\n",
    "    # For embeddings, we use 'nomic-embed-text', a top-tier, efficient open-source model.\n",
    "    \"embedding_model\": OllamaEmbeddings(model=\"snowflake-arctic-embed:22m\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56436bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM clients configured:\n",
      "Planner (gpt-oss:20b-cloud): model='gpt-oss:20b-cloud' temperature=0.0 format='json'\n",
      "Drafter (deepseek-v3.1:671b-cloud): model='deepseek-v3.1:671b-cloud' temperature=0.2\n",
      "SQL Coder (deepseek-v3.1:671b-cloud): model='deepseek-v3.1:671b-cloud' temperature=0.0\n",
      "Director (gpt-oss:120b-cloud): model='gpt-oss:120b-cloud' temperature=0.0 format='json'\n",
      "Embedding Model (snowflake-arctic-embed:22m): base_url='http://localhost:11434' model='snowflake-arctic-embed:22m' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n"
     ]
    }
   ],
   "source": [
    "# Print the configuration to confirm the clients are initialized and their parameters are set correctly.\n",
    "print(\"LLM clients configured:\")\n",
    "print(f\"Planner ({llm_config['planner'].model}): {llm_config['planner']}\")\n",
    "print(f\"Drafter ({llm_config['drafter'].model}): {llm_config['drafter']}\")\n",
    "print(f\"SQL Coder ({llm_config['sql_coder'].model}): {llm_config['sql_coder']}\")\n",
    "print(f\"Director ({llm_config['director'].model}): {llm_config['director']}\")\n",
    "print(f\"Embedding Model ({llm_config['embedding_model'].model}): {llm_config['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44123797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# A dictionary to hold the paths for our different data types. This keeps our file management clean and centralized.\n",
    "data_paths = {\n",
    "    \"base\": \"./data\",\n",
    "    \"pubmed\": \"./data/pubmed_articles\",\n",
    "    \"fda\": \"./data/fda_guidelines\",\n",
    "    \"ethics\": \"./data/ethical_guidelines\",\n",
    "    \"mimic\": \"./data/mimic_db\"\n",
    "}\n",
    "# This loop iterates through our defined paths and uses os.makedirs() to create any directories that don't already exist.\n",
    "# This prevents errors in later steps when we try to save files to these locations.\n",
    "for path in data_paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e7fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "\n",
    "def download_pubmed_articles(query, max_articles=20):\n",
    "    \"\"\"Fetches abstracts from PubMed for a given query and saves them as text files.\"\"\"\n",
    "    # The NCBI API requires an email address for identification. We fetch it from our environment variables.\n",
    "    Entrez.email = os.environ.get(\"ENTREZ_EMAIL\")\n",
    "    print(f\"Fetching PubMed articles for query: {query}\")\n",
    "    \n",
    "    # Step 1: Use Entrez.esearch to find the PubMed IDs (PMIDs) for articles matching our query.\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_articles, sort=\"relevance\")\n",
    "    record = Entrez.read(handle)\n",
    "    id_list = record[\"IdList\"]\n",
    "    print(f\"Found {len(id_list)} article IDs.\")\n",
    "    \n",
    "    print(\"Downloading articles...\")\n",
    "    # Step 2: Use Entrez.efetch to retrieve the full records (in MEDLINE format) for the list of PMIDs.\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = Medline.parse(handle)\n",
    "    \n",
    "    count = 0\n",
    "    # Step 3: Iterate through the retrieved records, parse them, and save each abstract to a file.\n",
    "    for i, record in enumerate(records):\n",
    "        pmid = record.get(\"PMID\", \"\")\n",
    "        title = record.get(\"TI\", \"No Title\")\n",
    "        abstract = record.get(\"AB\", \"No Abstract\")\n",
    "        if pmid:\n",
    "            # We name the file after the PMID for easy reference and to avoid duplicates.\n",
    "            filepath = os.path.join(data_paths[\"pubmed\"], f\"{pmid}.txt\")\n",
    "            with open(filepath, \"w\") as f:\n",
    "                f.write(f\"Title: {title}\\n\\nAbstract: {abstract}\")\n",
    "            print(f\"[{i+1}/{len(id_list)}] Fetching PMID: {pmid}... Saved to {filepath}\")\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b11e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching PubMed articles for query: (SGLT2 inhibitor) AND (type 2 diabetes) AND (renal impairment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshugangwar/Desktop/Anshu/LangGraph/.venv/lib/python3.12/site-packages/Bio/Entrez/__init__.py:734: UserWarning: \n",
      "            Email address is not specified.\n",
      "\n",
      "            To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "            email address with each request.  As an example, if your email address\n",
      "            is A.N.Other@example.com, you can specify it as follows:\n",
      "               from Bio import Entrez\n",
      "               Entrez.email = 'A.N.Other@example.com'\n",
      "            In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "            a user at the email address provided before blocking access to the\n",
      "            E-utilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 article IDs.\n",
      "Downloading articles...\n",
      "[1/20] Fetching PMID: 36945734... Saved to ./data/pubmed_articles/36945734.txt\n",
      "[2/20] Fetching PMID: 38914124... Saved to ./data/pubmed_articles/38914124.txt\n",
      "[3/20] Fetching PMID: 30697905... Saved to ./data/pubmed_articles/30697905.txt\n",
      "[4/20] Fetching PMID: 36335326... Saved to ./data/pubmed_articles/36335326.txt\n",
      "[5/20] Fetching PMID: 36351458... Saved to ./data/pubmed_articles/36351458.txt\n",
      "[6/20] Fetching PMID: 34619106... Saved to ./data/pubmed_articles/34619106.txt\n",
      "[7/20] Fetching PMID: 40327845... Saved to ./data/pubmed_articles/40327845.txt\n",
      "[8/20] Fetching PMID: 35113333... Saved to ./data/pubmed_articles/35113333.txt\n",
      "[9/20] Fetching PMID: 33413348... Saved to ./data/pubmed_articles/33413348.txt\n",
      "[10/20] Fetching PMID: 34272327... Saved to ./data/pubmed_articles/34272327.txt\n",
      "[11/20] Fetching PMID: 34817311... Saved to ./data/pubmed_articles/34817311.txt\n",
      "[12/20] Fetching PMID: 35145275... Saved to ./data/pubmed_articles/35145275.txt\n",
      "[13/20] Fetching PMID: 28432726... Saved to ./data/pubmed_articles/28432726.txt\n",
      "[14/20] Fetching PMID: 38913113... Saved to ./data/pubmed_articles/38913113.txt\n",
      "[15/20] Fetching PMID: 31101403... Saved to ./data/pubmed_articles/31101403.txt\n",
      "[16/20] Fetching PMID: 28904068... Saved to ./data/pubmed_articles/28904068.txt\n",
      "[17/20] Fetching PMID: 35977807... Saved to ./data/pubmed_articles/35977807.txt\n",
      "[18/20] Fetching PMID: 31196815... Saved to ./data/pubmed_articles/31196815.txt\n",
      "[19/20] Fetching PMID: 31857443... Saved to ./data/pubmed_articles/31857443.txt\n",
      "[20/20] Fetching PMID: 32862232... Saved to ./data/pubmed_articles/32862232.txt\n",
      "PubMed download complete. 20 articles saved.\n"
     ]
    }
   ],
   "source": [
    "# We define a specific, boolean query to find articles highly relevant to our trial concept.\n",
    "pubmed_query = \"(SGLT2 inhibitor) AND (type 2 diabetes) AND (renal impairment)\"\n",
    "num_downloaded = download_pubmed_articles(pubmed_query)\n",
    "print(f\"PubMed download complete. {num_downloaded} articles saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import io\n",
    "\n",
    "def download_and_extract_text_from_pdf(url, output_path):\n",
    "    \"\"\"Downloads a PDF from a URL, saves it, and also extracts its text content to a separate .txt file.\"\"\"\n",
    "    print(f\"Downloading FDA Guideline: {url}\")\n",
    "    try:\n",
    "        # We use the 'requests' library to perform the HTTP GET request to download the file.\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        # --- End of Change ---\n",
    "\n",
    "        # Send a GET request to the URL with the new headers\n",
    "        print(f\"Requesting URL: {url}\")\n",
    "        # The `requests` library will automatically follow redirects.\n",
    "        response = requests.get(url, stream=True, headers=headers, timeout=30)\n",
    "        response.raise_for_status() # This is a good practice that will raise an error if the download fails (e.g., a 404 error).\n",
    "        # We save the raw PDF file, which is useful for archival purposes.\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Successfully downloaded and saved to {output_path}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14a509a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This URL points to a real FDA guidance document for developing drugs for diabetes.\n",
    "fda_url = \"https://www.fda.gov/media/71185/download\"\n",
    "fda_pdf_path = os.path.join(data_paths[\"fda\"], \"fda_diabetes_guidance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64eebfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/fda_guidelines/fda_diabetes_guidance.pdf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fda_pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65c4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FDA Guideline: https://www.fda.gov/media/71185/download\n",
      "Requesting URL: https://www.fda.gov/media/71185/download\n",
      "Error downloading file: 404 Client Error: Not Found for url: https://www.fda.gov/apology_objects/abuse-detection-apology.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_and_extract_text_from_pdf(fda_url, fda_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18a4daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    " # We then use pypdf to read the PDF content directly from the in-memory response.\n",
    "reader = PdfReader(fda_pdf_path)\n",
    "text = \"\"\n",
    "# We loop through each page of the PDF and append its extracted text.\n",
    "for page in reader.pages:\n",
    "    print(page.get_contents())\n",
    "    text += page.extract_text() + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f70415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we save the clean, extracted text to a .txt file. This is the file our RAG system will actually use.\n",
    "txt_output_path = os.path.splitext(fda_pdf_path)[0] + '.txt'\n",
    "with open(txt_output_path, 'w') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1e33a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ethics guideline file: ./data/ethical_guidelines/belmont_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# This multi-line string contains a curated summary of the three core principles of the Belmont Report,\n",
    "# which is the foundational document for ethics in human subject research in the United States.\n",
    "ethics_content = \"\"\"\n",
    "Title: Summary of the Belmont Report Principles for Clinical Research\n",
    "1. Respect for Persons: This principle requires that individuals be treated as autonomous agents and that persons with diminished autonomy are entitled to protection. This translates to robust informed consent processes. Inclusion/exclusion criteria must not unduly target or coerce vulnerable populations, such as economically disadvantaged individuals, prisoners, or those with severe cognitive impairments, unless the research is directly intended to benefit that population.\n",
    "2. Beneficence: This principle involves two complementary rules: (1) do not harm and (2) maximize possible benefits and minimize possible harms. The criteria must be designed to select a population that is most likely to benefit and least likely to be harmed by the intervention. The risks to subjects must be reasonable in relation to anticipated benefits.\n",
    "3. Justice: This principle concerns the fairness of distribution of the burdens and benefits of research. The selection of research subjects must be equitable. Criteria should not be designed to exclude certain groups without a sound scientific or safety-related justification. For example, excluding participants based on race, gender, or socioeconomic status is unjust unless there is a clear rationale related to the drug's mechanism or risk profile.\n",
    "\"\"\"\n",
    "\n",
    "# We define the path where our ethics document will be saved.\n",
    "ethics_path = os.path.join(data_paths[\"ethics\"], \"belmont_summary.txt\")\n",
    "\n",
    "# We open the file in write mode and save the content.\n",
    "with open(ethics_path, \"w\") as f:\n",
    "    f.write(ethics_content)\n",
    "print(f\"Created ethics guideline file: {ethics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4204bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def load_real_mimic_data():\n",
    "    \"\"\"Loads real MIMIC-III CSVs into a persistent DuckDB database file, processing the massive LABEVENTS table efficiently.\"\"\"\n",
    "    print(\"Attempting to load real MIMIC-III data from local CSVs...\")\n",
    "    db_path = os.path.join(data_paths[\"mimic\"], \"mimic3_real.db\")\n",
    "    csv_dir = os.path.join(data_paths[\"mimic\"], \"mimiciii_csvs\")\n",
    "    \n",
    "    # Define the paths to the required compressed CSV files.\n",
    "    required_files = {\n",
    "        \"patients\": os.path.join(csv_dir, \"PATIENTS.csv.gz\"),\n",
    "        \"diagnoses\": os.path.join(csv_dir, \"DIAGNOSES_ICD.csv.gz\"),\n",
    "        \"labevents\": os.path.join(csv_dir, \"LABEVENTS.csv.gz\"),\n",
    "    }\n",
    "    \n",
    "    # Before starting, we check if all the necessary source files are present.\n",
    "    missing_files = [path for path in required_files.values() if not os.path.exists(path)]\n",
    "    if missing_files:\n",
    "        print(\"ERROR: The following MIMIC-III files were not found:\")\n",
    "        for f in missing_files: print(f\"- {f}\")\n",
    "        print(\"\\nPlease download them as instructed and place them in the correct directory.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Required files found. Proceeding with database creation.\")\n",
    "    # Remove any old database file to ensure we are building from scratch.\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "    # Connect to DuckDB. If the database file doesn't exist, it will be created.\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Use DuckDB's powerful `read_csv_auto` to directly load data from the gzipped CSVs into SQL tables.\n",
    "    print(f\"Loading {required_files['patients']} into DuckDB...\")\n",
    "    con.execute(f\"CREATE TABLE patients AS SELECT SUBJECT_ID, GENDER, DOB, DOD FROM read_csv_auto('{required_files['patients']}')\")\n",
    "    \n",
    "    print(f\"Loading {required_files['diagnoses']} into DuckDB...\")\n",
    "    con.execute(f\"CREATE TABLE diagnoses_icd AS SELECT SUBJECT_ID, ICD9_CODE FROM read_csv_auto('{required_files['diagnoses']}')\")\n",
    "    \n",
    "    # The LABEVENTS table is enormous. To handle it robustly, we use a two-stage process.\n",
    "    print(f\"Loading and processing {required_files['labevents']} (this may take several minutes)...\")\n",
    "    # 1. Load the data into a temporary 'staging' table, treating all columns as text (`all_varchar=True`).\n",
    "    #    This prevents parsing errors with mixed data types. We also filter for only the lab item IDs we\n",
    "    #    care about (50912 for Creatinine, 50852 for HbA1c) and use a regex to ensure VALUENUM is numeric.\n",
    "    con.execute(f\"\"\"CREATE TABLE labevents_staging AS \n",
    "                   SELECT SUBJECT_ID, ITEMID, VALUENUM \n",
    "                   FROM read_csv_auto('{required_files['labevents']}', all_varchar=True) \n",
    "                   WHERE ITEMID IN ('50912', '50852') AND VALUENUM IS NOT NULL AND VALUENUM ~ '^[0-9]+(\\\\.[0-9]+)?$'\n",
    "                \"\"\")\n",
    "    # 2. Create the final, clean table by selecting from the staging table and casting the columns to their correct numeric types.\n",
    "    con.execute(\"CREATE TABLE labevents AS SELECT SUBJECT_ID, CAST(ITEMID AS INTEGER) AS ITEMID, CAST(VALUENUM AS DOUBLE) AS VALUENUM FROM labevents_staging\")\n",
    "    # 3. Drop the temporary staging table to save space.\n",
    "    con.execute(\"DROP TABLE labevents_staging\")\n",
    "    con.close()\n",
    "    return db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f848a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load real MIMIC-III data from local CSVs...\n",
      "ERROR: The following MIMIC-III files were not found:\n",
      "- ./data/mimic_db/mimiciii_csvs/PATIENTS.csv.gz\n",
      "- ./data/mimic_db/mimiciii_csvs/DIAGNOSES_ICD.csv.gz\n",
      "- ./data/mimic_db/mimiciii_csvs/LABEVENTS.csv.gz\n",
      "\n",
      "Please download them as instructed and place them in the correct directory.\n"
     ]
    }
   ],
   "source": [
    "# Execute the function to build the database.\n",
    "db_path = load_real_mimic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b89ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load real MIMIC-III data from local CSVs...\n",
      "ERROR: The following MIMIC-III files were not found:\n",
      "- ./data/mimic_db/mimiciii_csvs/PATIENTS.csv.gz\n",
      "- ./data/mimic_db/mimiciii_csvs/DIAGNOSES_ICD.csv.gz\n",
      "- ./data/mimic_db/mimiciii_csvs/LABEVENTS.csv.gz\n",
      "\n",
      "Please download them as instructed and place them in the correct directory.\n"
     ]
    }
   ],
   "source": [
    "# If the database was created successfully, connect to it and inspect the schema and some sample data.\n",
    "if db_path:\n",
    "    print(f\"\\nReal MIMIC-III database created at: {db_path}\")\n",
    "    print(\"\\nTesting database connection and schema...\")\n",
    "    con = duckdb.connect(db_path)\n",
    "    print(f\"Tables in DB: {con.execute('SHOW TABLES').df()['name'].tolist()}\")\n",
    "    print(\"\\nSample of 'patients' table:\")\n",
    "    print(con.execute(\"SELECT * FROM patients LIMIT 5\").df())\n",
    "    print(\"\\nSample of 'diagnoses_icd' table:\")\n",
    "    print(con.execute(\"SELECT * FROM diagnoses_icd LIMIT 5\").df())\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
