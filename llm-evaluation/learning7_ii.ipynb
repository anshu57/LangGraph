{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccb096d",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09cf33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE, \n",
    "    llm_classify,\n",
    "    OpenAIModel\n",
    ")\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4ad91",
   "metadata": {},
   "source": [
    "You'll use `llm_classify` to define your LLM-as-a-judge evaluator. OpenAIModel is a class that wraps the OpenAI model, and you can use it to define and pass the model objects to `llm_classify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b979852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"evaluating-agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db1f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: evaluating-agent\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/gangwaranshu3/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import run_agent, start_main_span, tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef45b6",
   "metadata": {},
   "source": [
    "The utils file contains the same instrumented agent code that you worked on in the previous lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a6da8",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code>, <code>utils.py</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix ‚Äì Tips, Help, and Download\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d82b0",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> üö®\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Your results might differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc2191",
   "metadata": {},
   "source": [
    "## Running Agent with a Set of Testing Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90e120",
   "metadata": {},
   "source": [
    "To evaluate your agent's components, you will run the agent using a set of questions. For each question, you will collect spans and send them to Phoenix. Next to evaluate an agent component, you will query some specific spans and use them as your testing examples for your evaluators. Finally, you will upload the evaluated spans to Phoenix.\n",
    "\n",
    "<img src=\"images/traces.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"What was the average transaction value?\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180e33e",
   "metadata": {},
   "source": [
    "## Link to Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e5973",
   "metadata": {},
   "source": [
    "You can open this link to check out the Phoenix UI and observe the collected spans. You can use the same link to check out the results of the evaluations you'll run in this notebook. \n",
    "\n",
    "**Note**: \n",
    "- Since each notebook of this course runs in an isolated environment, each notebook links to a different Phoenix server. This is why you won't see the project \"tracing-agent\" you worked on in the previous notebook (as shown in the video).\n",
    "- Make sure that the notebook's kernel is running when checking the Phoenix UI. If the link does not open, it might be because the notebook has been open or inactive for a long time. In that case, make sure to refresh the browser, run all previous cells and then check this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_phoenix_endpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719f8be",
   "metadata": {},
   "source": [
    "## Router Evals using LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e2427",
   "metadata": {},
   "source": [
    "To evaluate the router, you will use this template provided by Phoenix to the LLM-as-a-Judge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOOL_CALLING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbbf7e",
   "metadata": {},
   "source": [
    "### Querying the Required Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    # Filter for the `LLM` span kind.\n",
    "    # The filter condition is a string of valid Python boolean expression.\n",
    "    \"span_kind == 'LLM'\",\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    tool_call=\"llm.tools\"\n",
    ")\n",
    "\n",
    "# The Phoenix Client can take this query and return the dataframe.\n",
    "tool_calls_df = px.Client().query_spans(query, \n",
    "                                        project_name=PROJECT_NAME, \n",
    "                                        timeout=None)\n",
    "tool_calls_df = tool_calls_df.dropna(subset=[\"tool_call\"])\n",
    "\n",
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0816c36",
   "metadata": {},
   "source": [
    "### Evaluating Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    tool_call_eval = llm_classify(\n",
    "        dataframe = tool_calls_df,\n",
    "        template = TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\"{tool_definitions}\", \n",
    "                                                                 json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"')),\n",
    "        rails = ['correct', 'incorrect'],\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78b133",
   "metadata": {},
   "source": [
    "## Evaluating Python Code Gen (Tool 3 - Data Visualization Evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e270e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"name =='generate_visualization'\"\n",
    ").select(\n",
    "    generated_code=\"output.value\"\n",
    ")\n",
    "\n",
    "# The Phoenix Client can take this query and return the dataframe.\n",
    "code_gen_df = px.Client().query_spans(query, \n",
    "                                      project_name=PROJECT_NAME, \n",
    "                                      timeout=None)\n",
    "\n",
    "code_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_is_runnable(output: str) -> bool:\n",
    "    \"\"\"Check if the code is runnable\"\"\"\n",
    "    output = output.strip()\n",
    "    output = output.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    try:\n",
    "        exec(output)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_df[\"label\"] = code_gen_df[\"generated_code\"].apply(code_is_runnable).map({True: \"runnable\", False: \"not_runnable\"})\n",
    "code_gen_df[\"score\"] = code_gen_df[\"label\"].map({\"runnable\": 1, \"not_runnable\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa60fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Runnable Code Eval\", dataframe=code_gen_df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc874af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f910c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2ab6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f537bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
